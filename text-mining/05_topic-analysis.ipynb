{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"text_english.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sent_tokenize(text)\n",
    "sent = pd.DataFrame({\"Sentence\": sent})\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token)>3:\n",
    "            result.append(stemmer.stem(token))\n",
    "    return result\n",
    "\n",
    "processed_sent = sent[\"Sentence\"].map(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [propos, unifi, neural, network, architectur, ...\n",
       "1     [versatil, achiev, tri, avoid, task, specif, e...\n",
       "2     [instead, exploit, input, featur, care, optim,...\n",
       "3     [work, basi, build, freeli, avail, tag, good, ...\n",
       "4                              [keyword, neuralnetwork]\n",
       "                            ...                        \n",
       "89    [later, ando, zhang, reach, semi, supervis, ap...\n",
       "90    [train, joint, linear, model, linear, model, a...\n",
       "91                [perform, viterbi, decod, test, time]\n",
       "92               [unlabel, corpus, word, taken, reuter]\n",
       "93    [featur, includ, word, tag, suffix, prefix, ch...\n",
       "Name: Sentence, Length: 94, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = gensim.corpora.Dictionary(processed_sent)\n",
    "d.filter_extremes(no_below=3, no_above=0.5, keep_n=100000)\n",
    "\n",
    "corpus = [d.doc2bow(doc) for doc in processed_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1)],\n",
       " [(14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
       " [(6, 1),\n",
       "  (14, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1)],\n",
       " [(13, 1), (27, 1)],\n",
       " [],\n",
       " [(5, 1), (8, 1), (20, 1), (28, 1), (29, 1), (30, 2)],\n",
       " [(20, 1), (29, 1)],\n",
       " [(24, 1), (31, 1)],\n",
       " [(5, 1), (8, 1), (19, 1), (24, 1), (31, 1), (32, 1)],\n",
       " [(1, 1),\n",
       "  (2, 1),\n",
       "  (4, 1),\n",
       "  (7, 1),\n",
       "  (10, 1),\n",
       "  (11, 2),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (31, 2),\n",
       "  (32, 1),\n",
       "  (33, 1)],\n",
       " [(20, 1), (27, 1), (29, 1), (30, 1), (34, 1)],\n",
       " [(5, 1), (8, 1), (35, 1), (36, 1), (37, 1)],\n",
       " [(36, 1), (38, 1)],\n",
       " [],\n",
       " [(39, 1)],\n",
       " [],\n",
       " [(40, 1)],\n",
       " [],\n",
       " [(41, 1)],\n",
       " [],\n",
       " [(42, 1)],\n",
       " [(38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1)],\n",
       " [(14, 1), (34, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1)],\n",
       " [(34, 1)],\n",
       " [(14, 1), (21, 1), (34, 1), (35, 1), (44, 1), (45, 1)],\n",
       " [(14, 1),\n",
       "  (17, 1),\n",
       "  (19, 1),\n",
       "  (21, 1),\n",
       "  (24, 1),\n",
       "  (32, 1),\n",
       "  (36, 1),\n",
       "  (46, 1),\n",
       "  (47, 1)],\n",
       " [(21, 1), (34, 1)],\n",
       " [(18, 1), (36, 1), (48, 1), (49, 1)],\n",
       " [(19, 1), (27, 1), (35, 1), (50, 1)],\n",
       " [(5, 1), (8, 1), (27, 1), (51, 1)],\n",
       " [(14, 1), (16, 1), (19, 1), (35, 1)],\n",
       " [(6, 1), (22, 1), (23, 1), (24, 1), (46, 1)],\n",
       " [(6, 1), (23, 1), (24, 1), (35, 2), (46, 1), (47, 1)],\n",
       " [(14, 1), (16, 1), (17, 1), (18, 1), (19, 1), (21, 1), (49, 1)],\n",
       " [(14, 1),\n",
       "  (20, 1),\n",
       "  (22, 1),\n",
       "  (24, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (49, 1),\n",
       "  (52, 1),\n",
       "  (53, 1)],\n",
       " [(18, 1), (48, 1)],\n",
       " [],\n",
       " [(14, 1), (28, 1), (35, 1), (54, 1)],\n",
       " [(25, 1), (28, 1), (35, 1), (44, 1), (55, 1), (56, 1), (57, 1)],\n",
       " [(5, 1),\n",
       "  (14, 1),\n",
       "  (20, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (32, 1),\n",
       "  (44, 2),\n",
       "  (49, 1),\n",
       "  (53, 1),\n",
       "  (54, 1)],\n",
       " [(23, 1), (24, 1), (27, 1), (35, 1), (44, 1), (51, 1), (57, 1)],\n",
       " [(25, 1), (54, 1), (57, 1)],\n",
       " [(14, 1), (15, 1), (17, 1), (19, 1), (34, 1), (37, 1), (51, 1), (54, 1)],\n",
       " [],\n",
       " [(54, 1)],\n",
       " [],\n",
       " [(1, 2),\n",
       "  (2, 1),\n",
       "  (4, 1),\n",
       "  (7, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 2),\n",
       "  (35, 2),\n",
       "  (37, 1),\n",
       "  (54, 1)],\n",
       " [(34, 1), (37, 1), (45, 1), (58, 2)],\n",
       " [(34, 1), (45, 1), (56, 1), (58, 1)],\n",
       " [(4, 1),\n",
       "  (10, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (28, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (35, 1),\n",
       "  (37, 1),\n",
       "  (58, 1)],\n",
       " [],\n",
       " [(20, 1), (25, 1), (54, 3), (59, 1), (60, 1)],\n",
       " [(0, 1),\n",
       "  (25, 1),\n",
       "  (30, 1),\n",
       "  (61, 1),\n",
       "  (62, 1),\n",
       "  (63, 1),\n",
       "  (64, 2),\n",
       "  (65, 1),\n",
       "  (66, 1),\n",
       "  (67, 1)],\n",
       " [(3, 1), (21, 1), (32, 1)],\n",
       " [],\n",
       " [],\n",
       " [(21, 1), (32, 1)],\n",
       " [],\n",
       " [(32, 1), (52, 1), (63, 1), (64, 1), (66, 1)],\n",
       " [(15, 1),\n",
       "  (25, 1),\n",
       "  (30, 1),\n",
       "  (48, 1),\n",
       "  (63, 1),\n",
       "  (65, 1),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (68, 1)],\n",
       " [(32, 1), (55, 1)],\n",
       " [],\n",
       " [(0, 1), (6, 2), (45, 1), (63, 1)],\n",
       " [(1, 2),\n",
       "  (4, 1),\n",
       "  (33, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 1)],\n",
       " [(1, 2), (32, 1)],\n",
       " [(1, 1), (14, 1), (20, 1), (25, 1), (54, 2), (59, 1), (69, 1)],\n",
       " [(15, 1), (25, 1), (60, 1)],\n",
       " [(1, 1), (69, 1), (70, 1), (71, 1)],\n",
       " [(61, 1)],\n",
       " [(13, 1), (21, 1), (25, 1), (32, 2), (67, 1)],\n",
       " [(27, 1), (59, 1), (72, 1)],\n",
       " [(13, 1), (25, 1), (51, 1), (54, 1), (64, 1)],\n",
       " [(34, 1), (56, 2), (61, 1), (71, 1)],\n",
       " [(13, 2), (21, 1), (32, 1), (34, 1)],\n",
       " [(24, 1), (25, 1), (55, 1), (64, 2)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(1, 1), (13, 1), (20, 1), (21, 2), (24, 2), (32, 1), (50, 1)],\n",
       " [(21, 1), (59, 1), (65, 1), (68, 1), (72, 1)],\n",
       " [(2, 1), (4, 1), (7, 1), (9, 1)],\n",
       " [(1, 1), (2, 1), (14, 1), (32, 1), (73, 1)],\n",
       " [(20, 2), (35, 1), (58, 1), (61, 1), (69, 1)],\n",
       " [(25, 1), (53, 1), (59, 1), (60, 1)],\n",
       " [],\n",
       " [(62, 1), (69, 1), (70, 1), (71, 1)],\n",
       " [(6, 1), (64, 1)],\n",
       " [(1, 1),\n",
       "  (3, 1),\n",
       "  (13, 2),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (25, 1),\n",
       "  (32, 1),\n",
       "  (49, 1),\n",
       "  (53, 1),\n",
       "  (64, 1),\n",
       "  (70, 1),\n",
       "  (73, 1)],\n",
       " [(21, 1), (27, 2), (50, 1), (62, 1), (69, 1)],\n",
       " [(48, 1), (52, 1), (57, 1)],\n",
       " [(14, 1), (25, 1), (44, 2)],\n",
       " [(27, 1), (59, 1), (65, 1), (68, 1), (72, 1)],\n",
       " [(26, 1), (32, 1)],\n",
       " [(1, 1), (3, 1), (13, 2), (21, 1), (32, 1), (69, 1), (70, 1), (73, 1)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.088*\"label\" + 0.073*\"chunk\" + 0.068*\"semant\" + 0.068*\"role\" + 0.068*\"speech\" + 0.067*\"inform\" + 0.058*\"tag\" + 0.055*\"benchmark\" + 0.055*\"entiti\" + 0.055*\"name\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.082*\"text\" + 0.082*\"test\" + 0.082*\"time\" + 0.081*\"perform\" + 0.079*\"structur\" + 0.079*\"data\" + 0.056*\"decod\" + 0.056*\"viterbi\" + 0.030*\"describ\" + 0.030*\"languag\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.212*\"section\" + 0.114*\"task\" + 0.071*\"train\" + 0.066*\"benchmark\" + 0.048*\"data\" + 0.048*\"test\" + 0.047*\"recognit\" + 0.040*\"chunk\" + 0.025*\"conll\" + 0.025*\"valid\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.062*\"perform\" + 0.060*\"bidirect\" + 0.058*\"approach\" + 0.043*\"word\" + 0.041*\"larg\" + 0.041*\"learn\" + 0.041*\"window\" + 0.040*\"infer\" + 0.039*\"knowledg\" + 0.037*\"featur\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.080*\"tag\" + 0.067*\"challeng\" + 0.063*\"includ\" + 0.060*\"chunk\" + 0.053*\"classifi\" + 0.050*\"conll\" + 0.048*\"word\" + 0.046*\"featur\" + 0.034*\"train\" + 0.034*\"prefix\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.075*\"report\" + 0.071*\"collobert\" + 0.071*\"kavukcuoglu\" + 0.071*\"weston\" + 0.071*\"kuksa\" + 0.060*\"bottou\" + 0.052*\"model\" + 0.052*\"karlen\" + 0.045*\"system\" + 0.036*\"task\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.106*\"featur\" + 0.101*\"tag\" + 0.095*\"word\" + 0.070*\"represent\" + 0.069*\"train\" + 0.067*\"model\" + 0.040*\"task\" + 0.039*\"unlabel\" + 0.039*\"data\" + 0.036*\"improv\"\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.086*\"task\" + 0.086*\"specif\" + 0.065*\"achiev\" + 0.065*\"engin\" + 0.065*\"system\" + 0.064*\"avoid\" + 0.044*\"knowledg\" + 0.044*\"standard\" + 0.044*\"valid\" + 0.044*\"train\"\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.094*\"word\" + 0.075*\"chunk\" + 0.074*\"setup\" + 0.051*\"task\" + 0.050*\"state\" + 0.049*\"represent\" + 0.048*\"languag\" + 0.048*\"natur\" + 0.048*\"intermedi\" + 0.046*\"specif\"\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.108*\"represent\" + 0.107*\"classifi\" + 0.081*\"benchmark\" + 0.079*\"learn\" + 0.057*\"data\" + 0.056*\"base\" + 0.056*\"intern\" + 0.054*\"discov\" + 0.036*\"reach\" + 0.030*\"obtain\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus, num_topics=10, id2word=d, passes=5,\n",
    "                                      workers=3)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f'Topic: {idx} \\nWords: {topic}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.081*\"learn\" + 0.050*\"unlabel\" + 0.049*\"discov\" + 0.046*\"reach\" + 0.042*\"represent\" + 0.041*\"instead\" + 0.040*\"intern\" + 0.038*\"intermedi\" + 0.032*\"word\" + 0.029*\"bidirect\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.097*\"chunk\" + 0.068*\"conll\" + 0.067*\"word\" + 0.060*\"featur\" + 0.058*\"challeng\" + 0.048*\"best\" + 0.047*\"prefix\" + 0.047*\"score\" + 0.043*\"hand\" + 0.032*\"perform\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.085*\"kuksa\" + 0.063*\"research\" + 0.063*\"collobert\" + 0.036*\"infer\" + 0.036*\"window\" + 0.036*\"viterbi\" + 0.034*\"achiev\" + 0.034*\"bidirect\" + 0.034*\"decod\" + 0.034*\"text\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.079*\"natur\" + 0.074*\"languag\" + 0.070*\"perform\" + 0.068*\"benchmark\" + 0.065*\"improv\" + 0.065*\"specif\" + 0.042*\"standard\" + 0.033*\"represent\" + 0.033*\"hand\" + 0.028*\"inform\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.077*\"avoid\" + 0.068*\"knowledg\" + 0.067*\"specif\" + 0.062*\"engin\" + 0.054*\"inform\" + 0.052*\"task\" + 0.046*\"research\" + 0.045*\"larg\" + 0.034*\"represent\" + 0.031*\"report\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.083*\"time\" + 0.074*\"data\" + 0.072*\"base\" + 0.071*\"structur\" + 0.067*\"test\" + 0.060*\"perform\" + 0.052*\"viterbi\" + 0.048*\"decod\" + 0.029*\"text\" + 0.025*\"setup\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.145*\"section\" + 0.079*\"model\" + 0.073*\"train\" + 0.057*\"obtain\" + 0.046*\"supervis\" + 0.037*\"data\" + 0.036*\"task\" + 0.032*\"test\" + 0.031*\"perform\" + 0.029*\"word\"\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.092*\"tag\" + 0.084*\"featur\" + 0.084*\"classifi\" + 0.068*\"system\" + 0.065*\"word\" + 0.046*\"train\" + 0.045*\"includ\" + 0.034*\"valid\" + 0.033*\"text\" + 0.030*\"window\"\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.069*\"kavukcuoglu\" + 0.069*\"weston\" + 0.069*\"bottou\" + 0.041*\"karlen\" + 0.038*\"setup\" + 0.038*\"collobert\" + 0.038*\"kuksa\" + 0.037*\"label\" + 0.037*\"chunk\" + 0.036*\"benchmark\"\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.086*\"approach\" + 0.086*\"knowledg\" + 0.028*\"score\" + 0.027*\"challeng\" + 0.025*\"conll\" + 0.022*\"chunk\" + 0.013*\"system\" + 0.011*\"featur\" + 0.011*\"train\" + 0.011*\"word\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=d, passes=5,\n",
    "                                      workers=3)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f'Topic: {idx} \\nWords: {topic}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
